{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Errors raised in HUB.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CADuLZmrUYyK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b689b2df-bc5a-44da-9bbc-c3636038aebc"
      },
      "source": [
        "!pip install hub"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hub\n",
            "  Downloading hub-2.0.11-py3-none-any.whl (188 kB)\n",
            "\u001b[K     |████████████████████████████████| 188 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting humbug>=0.2.6\n",
            "  Downloading humbug-0.2.7-py3-none-any.whl (11 kB)\n",
            "Collecting types-click\n",
            "  Downloading types_click-7.1.5-py3-none-any.whl (12 kB)\n",
            "Collecting google-cloud-storage~=1.42.0\n",
            "  Downloading google_cloud_storage-1.42.3-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 60.2 MB/s \n",
            "\u001b[?25hCollecting lz4\n",
            "  Downloading lz4-3.1.3-cp37-cp37m-manylinux2010_x86_64.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 50.1 MB/s \n",
            "\u001b[?25hCollecting pillow==8.2.0\n",
            "  Downloading Pillow-8.2.0-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 33.3 MB/s \n",
            "\u001b[?25hCollecting typing-extensions>=3.10.0.0\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hub) (1.19.5)\n",
            "Collecting google-auth~=2.0.1\n",
            "  Downloading google_auth-2.0.2-py2.py3-none-any.whl (152 kB)\n",
            "\u001b[K     |████████████████████████████████| 152 kB 60.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib~=0.4.5 in /usr/local/lib/python3.7/dist-packages (from hub) (0.4.6)\n",
            "Collecting types-requests\n",
            "  Downloading types_requests-2.25.9-py3-none-any.whl (22 kB)\n",
            "Collecting boto3-stubs[essential]\n",
            "  Downloading boto3_stubs-1.18.58-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.5 MB/s \n",
            "\u001b[?25hCollecting pathos\n",
            "  Downloading pathos-0.2.8-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 9.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from hub) (4.62.3)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.18.58-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 72.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth~=2.0.1->hub) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth~=2.0.1->hub) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth~=2.0.1->hub) (0.2.8)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth~=2.0.1->hub) (57.4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib~=0.4.5->hub) (1.3.0)\n",
            "Collecting google-api-core<3.0dev,>=1.29.0\n",
            "  Downloading google_api_core-2.1.0-py2.py3-none-any.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage~=1.42.0->hub) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage~=1.42.0->hub) (1.15.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage~=1.42.0->hub) (2.23.0)\n",
            "Collecting google-resumable-media<3.0dev,>=1.3.0\n",
            "  Downloading google_resumable_media-2.0.3-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting google-cloud-core<3.0dev,>=1.6.0\n",
            "  Downloading google_cloud_core-2.1.0-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage~=1.42.0->hub) (1.53.0)\n",
            "Collecting google-crc32c<2.0dev,>=1.0\n",
            "  Downloading google_crc32c-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38 kB)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0.1->hub) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage~=1.42.0->hub) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage~=1.42.0->hub) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage~=1.42.0->hub) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage~=1.42.0->hub) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib~=0.4.5->hub) (3.1.1)\n",
            "Collecting botocore<1.22.0,>=1.21.58\n",
            "  Downloading botocore-1.21.58-py3-none-any.whl (8.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 21.9 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.6 MB/s \n",
            "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 64.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.58->boto3->hub) (2.8.2)\n",
            "Collecting botocore-stubs\n",
            "  Downloading botocore_stubs-1.21.58-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 186 kB/s \n",
            "\u001b[?25hCollecting mypy-boto3-sqs>=1.18.53\n",
            "  Downloading mypy_boto3_sqs-1.18.58-py3-none-any.whl (28 kB)\n",
            "Collecting mypy-boto3-dynamodb>=1.18.53\n",
            "  Downloading mypy_boto3_dynamodb-1.18.58-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting mypy-boto3-s3>=1.18.53\n",
            "  Downloading mypy_boto3_s3-1.18.58-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting mypy-boto3-lambda>=1.18.53\n",
            "  Downloading mypy_boto3_lambda-1.18.58-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 5.8 MB/s \n",
            "\u001b[?25hCollecting mypy-boto3-rds>=1.18.53\n",
            "  Downloading mypy_boto3_rds-1.18.58-py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting mypy-boto3-ec2>=1.18.53\n",
            "  Downloading mypy_boto3_ec2-1.18.58-py3-none-any.whl (308 kB)\n",
            "\u001b[K     |████████████████████████████████| 308 kB 52.2 MB/s \n",
            "\u001b[?25hCollecting mypy-boto3-cloudformation>=1.18.53\n",
            "  Downloading mypy_boto3_cloudformation-1.18.58-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess>=0.70.12 in /usr/local/lib/python3.7/dist-packages (from pathos->hub) (0.70.12.2)\n",
            "Collecting ppft>=1.6.6.4\n",
            "  Downloading ppft-1.6.6.4-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting pox>=0.3.0\n",
            "  Downloading pox-0.3.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.7/dist-packages (from pathos->hub) (0.3.4)\n",
            "Installing collected packages: urllib3, typing-extensions, jmespath, google-auth, google-crc32c, google-api-core, botocore-stubs, botocore, s3transfer, ppft, pox, mypy-boto3-sqs, mypy-boto3-s3, mypy-boto3-rds, mypy-boto3-lambda, mypy-boto3-ec2, mypy-boto3-dynamodb, mypy-boto3-cloudformation, google-resumable-media, google-cloud-core, boto3-stubs, types-requests, types-click, pillow, pathos, lz4, humbug, google-cloud-storage, boto3, hub\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.7.4.3\n",
            "    Uninstalling typing-extensions-3.7.4.3:\n",
            "      Successfully uninstalled typing-extensions-3.7.4.3\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 1.35.0\n",
            "    Uninstalling google-auth-1.35.0:\n",
            "      Successfully uninstalled google-auth-1.35.0\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 1.26.3\n",
            "    Uninstalling google-api-core-1.26.3:\n",
            "      Successfully uninstalled google-api-core-1.26.3\n",
            "  Attempting uninstall: google-resumable-media\n",
            "    Found existing installation: google-resumable-media 0.4.1\n",
            "    Uninstalling google-resumable-media-0.4.1:\n",
            "      Successfully uninstalled google-resumable-media-0.4.1\n",
            "  Attempting uninstall: google-cloud-core\n",
            "    Found existing installation: google-cloud-core 1.0.3\n",
            "    Uninstalling google-cloud-core-1.0.3:\n",
            "      Successfully uninstalled google-cloud-core-1.0.3\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 1.18.1\n",
            "    Uninstalling google-cloud-storage-1.18.1:\n",
            "      Successfully uninstalled google-cloud-storage-1.18.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires typing-extensions~=3.7.4, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "tensorboard 2.6.0 requires google-auth<2,>=1.6.3, but you have google-auth 2.0.2 which is incompatible.\n",
            "google-cloud-translate 1.5.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.1.0 which is incompatible.\n",
            "google-cloud-translate 1.5.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.1.0 which is incompatible.\n",
            "google-cloud-language 1.2.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.1.0 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.1.0 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.1.0 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.1.0 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.1.0 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.1.0 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1, but you have google-resumable-media 2.0.3 which is incompatible.\n",
            "google-cloud-bigquery-storage 1.1.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.1.0 which is incompatible.\n",
            "google-api-python-client 1.12.8 requires google-api-core<2dev,>=1.21.0, but you have google-api-core 2.1.0 which is incompatible.\n",
            "firebase-admin 4.4.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0; platform_python_implementation != \"PyPy\", but you have google-api-core 2.1.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.18.58 boto3-stubs-1.18.58 botocore-1.21.58 botocore-stubs-1.21.58 google-api-core-2.1.0 google-auth-2.0.2 google-cloud-core-2.1.0 google-cloud-storage-1.42.3 google-crc32c-1.3.0 google-resumable-media-2.0.3 hub-2.0.11 humbug-0.2.7 jmespath-0.10.0 lz4-3.1.3 mypy-boto3-cloudformation-1.18.58 mypy-boto3-dynamodb-1.18.58 mypy-boto3-ec2-1.18.58 mypy-boto3-lambda-1.18.58 mypy-boto3-rds-1.18.58 mypy-boto3-s3-1.18.58 mypy-boto3-sqs-1.18.58 pathos-0.2.8 pillow-8.2.0 pox-0.3.0 ppft-1.6.6.4 s3transfer-0.5.0 types-click-7.1.5 types-requests-2.25.9 typing-extensions-3.10.0.2 urllib3-1.25.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE15LG7eVPko"
      },
      "source": [
        "# **Attribute Error:** \n",
        "After installing hub package using pip, restart the runtime and then run 'import hub' command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH8t0AsIX3rL"
      },
      "source": [
        "import hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB_LeiwJa7_B"
      },
      "source": [
        "Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iAqQf4jYCkK",
        "outputId": "b6d2c31a-8e52-4e93-bf9e-d9e2acd91c13"
      },
      "source": [
        "# USING GPU\n",
        "dataset_path = 'hub://activeloop/mnist-train'\n",
        "ds = hub.load(dataset_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opening dataset in read-only mode as you don't have write permissions.\n",
            "hub://activeloop/mnist-train loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zomipMya_5l"
      },
      "source": [
        "# **ReadOnlyModeError:**\n",
        "The above command does not download dataset on local and is present only in read-only mode. So, if you want to make modifications, you have to first get it into writing format. \n",
        "Although you could try using the read_only parameter of `hub.load `instead of `hub.dataset` which toggles between read mode and write."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "1aWgnwxUZYVE",
        "outputId": "2189cd8d-453b-47ac-f06f-40d63b670cea"
      },
      "source": [
        "ps = hub.dataset(dataset_path)\n",
        "ps.create_tensor('test')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opening dataset in read-only mode as you don't have write permissions.\n",
            "hub://activeloop/mnist-train loaded successfully.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ReadOnlyModeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mReadOnlyModeError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-bf7b3ceead6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/humbug/report.py\u001b[0m in \u001b[0;36mwrapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/hub/core/dataset.py\u001b[0m in \u001b[0;36mcreate_tensor\u001b[0;34m(self, name, htype, dtype, sample_compression, chunk_compression, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mchunk_compression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_compression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mversion_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mmeta_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         )\n\u001b[1;32m    293\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"meta\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/hub/core/tensor.py\u001b[0m in \u001b[0;36mcreate_tensor\u001b[0;34m(key, storage, htype, sample_compression, chunk_compression, version_state, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     )\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mstorage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmeta_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/hub/core/storage/lru_cache.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, path, value)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__delitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/hub/core/storage/provider.py\u001b[0m in \u001b[0;36mmaybe_flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \"\"\"\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoflush\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/hub/core/storage/lru_cache.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirty_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirty_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_storage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/hub/core/storage/lru_cache.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, path, remove_from_dirty)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_storage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_storage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_from_dirty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_from_dirty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/hub/core/storage/lru_cache.py\u001b[0m in \u001b[0;36m_forward_value\u001b[0;34m(self, path, value, remove_from_dirty)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcachable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_storage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_storage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/hub/core/storage/s3.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, path, content)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mReadOnlyError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprovider\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0monly\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \"\"\"\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_readonly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_update_creds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/hub/core/storage/provider.py\u001b[0m in \u001b[0;36mcheck_readonly\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;34m\"\"\"Raises an exception if the provider is in read-only mode.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mReadOnlyModeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mReadOnlyModeError\u001b[0m: Modification when in read-only mode is not supported!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaxWE97NoY1k"
      },
      "source": [
        "ps = hub.load(dataset_path)\n",
        "ps.create_tensor('test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHVA1VGLzMOU"
      },
      "source": [
        "#To create dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZSK8YJAo3FE"
      },
      "source": [
        "def empty(path, overwrite=False, \n",
        "          public=True, memory_cache_size=256, \n",
        "          local_cache_size=0, creds=None, token=None)\n",
        "\n",
        "#DatasetHandlerError: If a Dataset already exists at the given path and overwrite is False.\n",
        "\n",
        "#path can be The full path to the dataset. Can be:-\n",
        "# - a Hub cloud path of the form hub://username/datasetname. To write to Hub cloud datasets, ensure that you are logged in to Hub (use 'activeloop login' from command line)\n",
        "# - an s3 path of the form s3://bucketname/path/to/dataset. Credentials are required in either the environment or passed to the creds argument.\n",
        "# - a local file system path of the form ./path/to/dataset or ~/path/to/dataset or path/to/dataset.\n",
        "# - a memory path of the form mem://path/to/dataset which doesn't save the dataset but keeps it in memory instead. Should be used only for testing as it does not persist."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZz1aadXJuxG"
      },
      "source": [
        "#Ingests a dataset from a source and stores it as a structured dataset to destination\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xFbmxjdKCa8"
      },
      "source": [
        "\n",
        "\n",
        "def ingest(src, dest, images_compression='auto', \n",
        "           dest_creds=None, progress_bar=True, \n",
        "           summary=True, **dataset_kwargs)\n",
        "\n",
        "#SamePathException: If the source and destination path(src) are same.\n",
        "#AutoCompressionError: If the source director(src, dest) is empty or does not contain a valid extension.\n",
        "#InvalidFileExtension: If the most frequent file extension is found to be 'None' during auto-compression.\n",
        "if images_compression == \"auto\":\n",
        "        images_compression = get_most_common_extension(src)\n",
        "        if images_compression is None:\n",
        "            raise InvalidFileExtension(src)\n",
        "#Invalid Path Exception: If the source directory(dest) does not exist.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLFROMLa4su8"
      },
      "source": [
        "#Download and ingest a kaggle dataset and store it as a structured dataset to destination\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56IJSZUq46UI"
      },
      "source": [
        "\n",
        "def ingest_kaggle(tag, src, dest, exist_ok=False, \n",
        "                  images_compression='auto', dest_creds=None, \n",
        "                  kaggle_credentials=None, progress_bar=True, summary=True, **dataset_kwargs)\n",
        "\n",
        "\n",
        "#SamePathException Error:If the source and destination path are same.\n",
        "\n",
        "# if os.path.isdir(src) and os.path.isdir(dest):\n",
        "#        if os.path.samefile(src, dest):\n",
        "#            raise SamePathException(src)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycBkn-nI6o--"
      },
      "source": [
        "#Loads an existing dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL4zLo-j84y3"
      },
      "source": [
        "def load(path, read_only=False, memory_cache_size=256, \n",
        "         local_cache_size=0, creds=None, token=None, verbose=True)\n",
        "\n",
        "#DatasetHandlerError: If a Dataset does not exist at the given path.\n",
        "# if not dataset_exists(storage):\n",
        "#        raise DatasetHandlerError(\n",
        "#            f\"A Hub dataset does not exist at the given path ({path}). Check the path provided or in case you want to create a new dataset, use hub.empty().\"\n",
        "#        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MRkdgLE_BAS"
      },
      "source": [
        "#Initializes a new or existing dataset in Classes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1xb1-sO_VXz"
      },
      "source": [
        "class dataset_cl \n",
        "(\n",
        "storage, index=None, group_index='', read_only=False, public=True, token=None, verbose=True, version_state=None, **kwargs)\n",
        "\n",
        "#Errors Raised:\n",
        "#ValueError: If an existing local path is given, it must be a directory.\n",
        "#ImproperDatasetInitialization: Exactly one argument out of 'path' and 'storage' needs to be specified. This is raised if none of them are specified or more than one are specifed.\n",
        "#InvalidHubPathException: If a Hub cloud path (path starting with hub://) is specified and it isn't of the form hub://username/datasetname.\n",
        "#AuthorizationException: If a Hub cloud path (path starting with hub://) is specified and the user doesn't have access to the dataset.\n",
        "#PathNotEmptyException: If the path to the dataset doesn't contain a Hub dataset and is also not empty."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJulXrrWA6Zl"
      },
      "source": [
        "#Creates a tensor group. Intermediate groups in the path are also created.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "COEWRYpoBeAn"
      },
      "source": [
        "#@title Default title text\n",
        "def create_tensor(self, name, htype='generic', \n",
        "                  dtype='unspecified', sample_compression='unspecified', \n",
        "                  chunk_compression='unspecified', **kwargs)\n",
        "\n",
        "\n",
        "##Errors Raised:\n",
        "\n",
        "#TensorAlreadyExistsError: Duplicate tensors are not allowed.\n",
        "#TensorGroupAlreadyExistsError: Duplicate tensor groups are not allowed.\n",
        "#InvalidTensorNameError: If name is in dataset attributes.\n",
        "#NotImplementedError: If trying to override 'chunk_compression'.\n",
        "    \n",
        " \"\"   # if not the head node, checkout to an auto branch that is newly created\n",
        "    auto_checkout(self.version_state, self.storage)\n",
        "    name = name.strip(\"/\")\n",
        "\n",
        "    while \"//\" in name:\n",
        "        name = name.replace(\"//\", \"/\")\n",
        "\n",
        "    full_path = posixpath.join(self.group_index, name)\n",
        "\n",
        "    if tensor_exists(full_path, self.storage, self.version_state[\"commit_id\"]):\n",
        "        raise TensorAlreadyExistsError(name)\n",
        "\n",
        "    if full_path in self._groups:\n",
        "        raise TensorGroupAlreadyExistsError(name)\n",
        "\n",
        "    if not name or name in dir(self):\n",
        "        raise InvalidTensorNameError(name)  \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VkuKk10Gisq"
      },
      "source": [
        "#Initializes a new tensor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7pMXVxNGJS_"
      },
      "source": [
        "class tensor \n",
        "(key, storage, version_state, index=None)\n",
        "\n",
        "#TensorDoesNotExistError: If no tensor with key exists and a tensor_meta was not provided.\n",
        ">> key (str): The internal identifier for this tensor."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVmtiwB__FaP"
      },
      "source": [
        "# Extends the end of the tensor by appending multiple elements from a sequence. Accepts a sequence, a single batched numpy array, or a sequence of `read()` outputs, which can be used to load files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLYeMDAx_MHh"
      },
      "source": [
        "def extend(self, samples: Union[np.ndarray, Sequence[SampleValue]]):\n",
        "    \"\"\"Extends the end of the tensor by appending multiple elements from a sequence. Accepts a sequence, a single batched numpy array,\n",
        "    or a sequence of `hub.read` outputs, which can be used to load files. See examples down below.\n",
        "\n",
        "    Example:\n",
        "        numpy input:\n",
        "            >>> len(tensor)\n",
        "            0\n",
        "            >>> tensor.extend(np.zeros((100, 28, 28, 1)))\n",
        "            >>> len(tensor)\n",
        "            100\n",
        "\n",
        "        file input:\n",
        "            >>> len(tensor)\n",
        "            0\n",
        "            >>> tensor.extend([\n",
        "                    hub.read(\"path/to/image1\"),\n",
        "                    hub.read(\"path/to/image2\"),\n",
        "                ])\n",
        "            >>> len(tensor)\n",
        "            2 \"\"\"\n",
        "\n",
        "#Error Raises:\n",
        "#TensorDtypeMismatchError: TensorDtypeMismatchError: Dtype for array must be equal to or castable to this tensor's dtype\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWK_nmM9E2kF"
      },
      "source": [
        "# Computes the contents of the tensor in numpy format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc4MonToE1xL"
      },
      "source": [
        " def numpy(self, aslist=False) \n",
        "\n",
        " \"\"\"Computes the contents of the tensor in numpy format.\n",
        "\n",
        "    Args:\n",
        "        aslist (bool): If True, a list of np.ndarrays will be returned. Helpful for dynamic tensors.\n",
        "            If False, a single np.ndarray will be returned unless the samples are dynamically shaped, in which case\n",
        "            an error is raised.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        A numpy array containing the data represented by this tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    return self.chunk_engine.numpy(self.index, aslist=aslist)\n",
        "\n",
        "#Error Raises: \n",
        "#DynamicTensorNumpyError: If reading a dynamically-shaped array slice without `aslist=True`.\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}