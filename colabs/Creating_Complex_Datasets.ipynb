{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Creating Complex Datasets",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/activeloopai/examples/blob/main/colabs/Creating_Complex_Datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKU8kmSs65xv"
      },
      "source": [
        "# ***Creating Complex Detection Datasets***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zK9b4yiMRzB"
      },
      "source": [
        "#### Datasets often have multiple labels such as classifications, bounding boxes, segmentations, and others.  In order to create an intuitive layout of tensors, it's advisable to create a dataset hierarchy that captures the relationship between the different label types. This can be done with hub tensor `groups`.\n",
        "\n",
        "#### This example show to to use groups to create a dataset containing image classifications of \"indoor\" and \"outdoor\", as well as bounding boxes of objects such as \"dog\" and \"cat\". "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UseHLcoRIYz"
      },
      "source": [
        "## Install Hub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5mOffq5RN-T"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "!pip3 install hub\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wGo53ndMTCB"
      },
      "source": [
        "## Create the Hub Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52h9xKujOJFs"
      },
      "source": [
        "The first step is to download the small dataset below called *animals complex*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6m__biyt5I1"
      },
      "source": [
        "# Download dataset\n",
        "from IPython.display import clear_output\n",
        "!wget https://github.com/activeloopai/examples/raw/main/colabs/starting_data/animals_complex.zip\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fNxNZIft5F-"
      },
      "source": [
        "# Unzip to './animals_complex' folder\n",
        "!unzip -qq /content/animals_complex.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLh4uuIMuNwt"
      },
      "source": [
        "The dataset has the following folder structure:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHyrqNgNuRO2"
      },
      "source": [
        "animals_complex\n",
        "- classification\n",
        "  - indoor\n",
        "    - image1.png\n",
        "    - image2.png\n",
        "  - outdoor\n",
        "    - image3.png\n",
        "    - image4.png\n",
        "- boxes\n",
        "  - image1.txt\n",
        "  - image3.txt\n",
        "  - image3.txt\n",
        "  - image4.txt\n",
        "  - classes.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_iOi_9NuXAI"
      },
      "source": [
        "Now that you have the data, let's create a Hub Dataset in the `./animals_complex_hub` folder by running: \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaZtpnpTOp-5"
      },
      "source": [
        "import hub\n",
        "from PIL import Image, ImageDraw\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "ds = hub.empty('./animals_complex_hub') # Create the dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNMOv3LPOyAd"
      },
      "source": [
        "Next, let's specify the folder paths containing the classification and object detection data. It's also helpful to create a list of all of the image files and class names for classification and object detection tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCjN0EKwO1Pu"
      },
      "source": [
        "classification_folder = './animals_complex/classification'\n",
        "boxes_folder = './animals_complex/boxes'\n",
        "\n",
        "# List of all class names for classification\n",
        "class_names = os.listdir(classification_folder)\n",
        "\n",
        "fn_imgs = []\n",
        "for dirpath, dirnames, filenames in os.walk(classification_folder):\n",
        "    for filename in filenames:\n",
        "        fn_imgs.append(os.path.join(dirpath, filename))\n",
        "\n",
        "# List of all class names for object detection        \n",
        "with open(os.path.join(boxes_folder, 'classes.txt'), 'r') as f:\n",
        "    class_names_boxes = f.read().splitlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4CPD4nmO3_S"
      },
      "source": [
        "Since annotations in YOLO are typically stored in text files, it's useful to write a helper function that parses the annotation file and returns numpy arrays with the bounding box coordinates and bounding box classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRIDfYXNO7kg"
      },
      "source": [
        "def read_yolo_boxes(fn:str):\n",
        "    \"\"\"\n",
        "    Function reads a label.txt YOLO file and returns a numpy array of yolo_boxes \n",
        "    for the box geometry and yolo_labels for the corresponding box labels.\n",
        "    \"\"\"\n",
        "    \n",
        "    box_f = open(fn)\n",
        "    lines = box_f.read()\n",
        "    box_f.close()\n",
        "    \n",
        "    # Split each box into a separate lines\n",
        "    lines_split = lines.splitlines()\n",
        "    \n",
        "    yolo_boxes = np.zeros((len(lines_split),4))\n",
        "    yolo_labels = np.zeros(len(lines_split))\n",
        "    \n",
        "    # Go through each line and parse data\n",
        "    for l, line in enumerate(lines_split):\n",
        "        line_split = line.split()\n",
        "        yolo_boxes[l,:]=np.array((float(line_split[1]), float(line_split[2]), float(line_split[3]), float(line_split[4])))\n",
        "        yolo_labels[l]=int(line_split[0]) \n",
        "         \n",
        "    return yolo_boxes, yolo_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKvPUjxcUPvO"
      },
      "source": [
        "Next, let's create the groups and tensors for this data. In order to separate the two annotations, a `boxes` group is created to wrap around the `label` and `bbox` tensors which contains the coordinates and labels for the bounding boxes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2F4TXd0UEtH"
      },
      "source": [
        "with ds:\n",
        "    # Image\n",
        "    ds.create_tensor('images', htype='image', sample_compression='jpeg')\n",
        "    \n",
        "    # Classification\n",
        "    ds.create_tensor('labels', htype='class_label', class_names = class_names)\n",
        "    \n",
        "    # Object Detection\n",
        "    ds.create_group('boxes')\n",
        "    ds.boxes.create_tensor('bbox', htype='bbox')\n",
        "    ds.boxes.create_tensor('label', htype='class_label', class_names = class_names_boxes)\n",
        "    # An alternate approach is to use '/' notation, which automatically creates the boxes group\n",
        "    # ds.create_tensor('boxes/bbox', ...)\n",
        "    # ds.create_tensor('boxes/label', ...)\n",
        "\n",
        "    # Define the format of the bounding boxes\n",
        "    ds.boxes.bbox.info.update(coords = {'type': 'fractional', 'mode': 'LTWH'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlCH5Xy8NTvx"
      },
      "source": [
        "Finally, let's iterate through all the images in the dataset in order to populate the data in Hub. The first axis of the `boxes.bbox` sample array corresponds to the first-and-only axis of the `boxes.label` sample array (i.e. if there are 3 boxes in an image, the labels array is 3x1 and the boxes array is 3x4)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz5lsbCeNUDg"
      },
      "source": [
        "with ds:\n",
        "    #Iterate throgh the images\n",
        "    for fn_img in fn_imgs:\n",
        "        \n",
        "        img_name = os.path.splitext(os.path.basename(fn_img))[0]\n",
        "        fn_box = img_name+'.txt'\n",
        "        \n",
        "        # Get the class number for the classification\n",
        "        label_text = os.path.basename(os.path.dirname(fn_img))\n",
        "        label_num = class_names.index(label_text)\n",
        "    \n",
        "        # Get the arrays for the bounding boxes and their classes\n",
        "        yolo_boxes, yolo_labels = read_yolo_boxes(os.path.join(boxes_folder,fn_box))\n",
        "        \n",
        "        # Append data to tensors\n",
        "        ds.append({'images': hub.read(os.path.join(fn_img)),\n",
        "                   'labels': np.uint32(label_num),\n",
        "                   'boxes/label': yolo_labels.astype(np.uint32),\n",
        "                   'boxes/bbox': yolo_boxes.astype(np.float32)\n",
        "        })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYCI61o-O9CV"
      },
      "source": [
        "##Inspect the Hub Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXkD-gLgO_7L"
      },
      "source": [
        "Let's check out the third sample from this dataset, which contains two bounding boxes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEPTKmCiPD-T"
      },
      "source": [
        "# Draw bounding boxes and the classfication label for the second image\n",
        "\n",
        "ind = 1\n",
        "img = Image.fromarray(ds.images[ind].numpy())\n",
        "draw = ImageDraw.Draw(img)\n",
        "(w,h) = img.size\n",
        "boxes = ds.boxes.bbox[ind].numpy()\n",
        "\n",
        "for b in range(boxes.shape[0]):\n",
        "    (xc,yc) = (int(boxes[b][0]*w), int(boxes[b][1]*h))\n",
        "    (x1,y1) = (int(xc-boxes[b][2]*w/2), int(yc-boxes[b][3]*h/2))\n",
        "    (x2,y2) = (int(xc+boxes[b][2]*w/2), int(yc+boxes[b][3]*h/2))\n",
        "    draw.rectangle([x1,y1,x2,y2], width=2)\n",
        "    draw.text((x1,y1), ds.boxes.label.info.class_names[ds.boxes.label[ind].numpy()[b]])\n",
        "    draw.text((0,0), ds.labels.info.class_names[ds.labels[ind].numpy()[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZMcRLeQPHq6"
      },
      "source": [
        "# Display the image and its bounding boxes\n",
        "img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79QnkE-UUySP"
      },
      "source": [
        "Congrats! You just created a dataset with multiple types of annotations! ðŸŽ‰"
      ]
    }
  ]
}