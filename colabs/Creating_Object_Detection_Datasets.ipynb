{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/activeloopai/examples/blob/main/colabs/Creating_Object_Detection_Datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKU8kmSs65xv"
      },
      "source": [
        "# ***Creating Object Detection Datasets***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zK9b4yiMRzB"
      },
      "source": [
        "#### Object detection and image annotation using bounding boxes is one of the most common data types for Computer Vision datasets. This tutorial demonstrates how to convert an object detection dataset in YOLO format into Hub, and a similar process can be used for uploading object detection data in other formats."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UseHLcoRIYz"
      },
      "source": [
        "## Install Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5mOffq5RN-T"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "!pip3 install hub\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wGo53ndMTCB"
      },
      "source": [
        "## Create the Hub Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52h9xKujOJFs"
      },
      "source": [
        "The first step is to download the small dataset below called *animals object detection*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6m__biyt5I1"
      },
      "outputs": [],
      "source": [
        "# Download dataset\n",
        "from IPython.display import clear_output\n",
        "!wget https://github.com/activeloopai/examples/raw/main/colabs/starting_data/animals_od.zip\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fNxNZIft5F-"
      },
      "outputs": [],
      "source": [
        "# Unzip to './animals_od' folder\n",
        "!unzip -qq /content/animals_od.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLh4uuIMuNwt"
      },
      "source": [
        "The dataset has the following folder structure:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHyrqNgNuRO2"
      },
      "source": [
        "animals_od\n",
        "- images\n",
        "  - image_1.jpg\n",
        "  - image_2.jpg\n",
        "  - image_3.jpg\n",
        "  - image_4.jpg\n",
        "- boxes\n",
        "  - image_1.txt\n",
        "  - image_2.txt\n",
        "  - image_3.txt\n",
        "  - image_4.txt\n",
        "  - classes.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_iOi_9NuXAI"
      },
      "source": [
        "Now that you have the data, let's **create a Hub Dataset** in the `./animals_od_hub` folder by running:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaZtpnpTOp-5"
      },
      "outputs": [],
      "source": [
        "import hub\n",
        "from PIL import Image, ImageDraw\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "ds = hub.empty('./animals_od_hub') # Create the dataset locally"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNMOv3LPOyAd"
      },
      "source": [
        "Next, let's specify the folder paths containing the images and annotations in the dataset. In YOLO format, images and annotations are typically matched using a common filename such as `image -> filename.jpeg` and `annotation -> filename.txt` . It's also helpful to create a list of all of the image files and the class names contained in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCjN0EKwO1Pu"
      },
      "outputs": [],
      "source": [
        "img_folder = './animals_od/images'\n",
        "lbl_folder = './animals_od/boxes'\n",
        "\n",
        "# List of all images\n",
        "fn_imgs = os.listdir(img_folder)\n",
        "\n",
        "# List of all class names\n",
        "with open(os.path.join(lbl_folder, 'classes.txt'), 'r') as f:\n",
        "    class_names = f.read().splitlines()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4CPD4nmO3_S"
      },
      "source": [
        "Since annotations in YOLO are typically stored in text files, it's useful to write a helper function that parses the annotation file and returns numpy arrays with the bounding box coordinates and bounding box classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRIDfYXNO7kg"
      },
      "outputs": [],
      "source": [
        "def read_yolo_boxes(fn:str):\n",
        "    \"\"\"\n",
        "    Function reads a label.txt YOLO file and returns a numpy array of yolo_boxes \n",
        "    for the box geometry and yolo_labels for the corresponding box labels.\n",
        "    \"\"\"\n",
        "    \n",
        "    box_f = open(fn)\n",
        "    lines = box_f.read()\n",
        "    box_f.close()\n",
        "    \n",
        "    # Split each box into a separate lines\n",
        "    lines_split = lines.splitlines()\n",
        "    \n",
        "    yolo_boxes = np.zeros((len(lines_split),4))\n",
        "    yolo_labels = np.zeros(len(lines_split))\n",
        "    \n",
        "    # Go through each line and parse data\n",
        "    for l, line in enumerate(lines_split):\n",
        "        line_split = line.split()\n",
        "        yolo_boxes[l,:]=np.array((float(line_split[1]), float(line_split[2]), float(line_split[3]), float(line_split[4])))\n",
        "        yolo_labels[l]=int(line_split[0]) \n",
        "         \n",
        "    return yolo_boxes, yolo_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKvPUjxcUPvO"
      },
      "source": [
        "Finally, let's create the tensors and iterate through all the images in the dataset in order to populate the data in Hub. Boxes and their labels will be stored in separate tensors, and for a given sample, the first axis of the boxes array corresponds to the first-and-only axis of the labels array (i.e. if there are 3 boxes in an image, the labels array is 3x1 and the boxes array is 3x4)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2F4TXd0UEtH"
      },
      "outputs": [],
      "source": [
        "with ds:\n",
        "    ds.create_tensor('images', htype='image', sample_compression = 'jpeg')\n",
        "    ds.create_tensor('labels', htype='class_label', class_names = class_names)\n",
        "    ds.create_tensor('boxes', htype='bbox')\n",
        "\n",
        "    # Define the format of the bounding boxes\n",
        "    ds.boxes.info.update(coords = {'type': 'fractional', 'mode': 'LTWH'})\n",
        "\n",
        "    for fn_img in fn_imgs:\n",
        "\n",
        "        img_name = os.path.splitext(fn_img)[0]\n",
        "        fn_box = img_name+'.txt'\n",
        "\n",
        "        # Get the arrays for the bounding boxes and their classes\n",
        "        yolo_boxes, yolo_labels = read_yolo_boxes(os.path.join(lbl_folder,fn_box))\n",
        "        \n",
        "        # Append data to tensors\n",
        "        ds.append({'images': hub.read(os.path.join(img_folder, fn_img)),\n",
        "                   'labels': yolo_labels.astype(np.uint32),\n",
        "                   'boxes': yolo_boxes.astype(np.float32)\n",
        "                   })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYCI61o-O9CV"
      },
      "source": [
        "##Inspect the Hub Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXkD-gLgO_7L"
      },
      "source": [
        "Let's check out the third sample from this dataset, which contains two bounding boxes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEPTKmCiPD-T"
      },
      "outputs": [],
      "source": [
        "# Draw bounding boxes for the fourth image\n",
        "\n",
        "ind = 3\n",
        "img = Image.fromarray(ds.images[ind ].numpy())\n",
        "draw = ImageDraw.Draw(img)\n",
        "(w,h) = img.size\n",
        "boxes = ds.boxes[ind ].numpy()\n",
        "\n",
        "for b in range(boxes.shape[0]):\n",
        "    (xc,yc) = (int(boxes[b][0]*w), int(boxes[b][1]*h))\n",
        "    (x1,y1) = (int(xc-boxes[b][2]*w/2), int(yc-boxes[b][3]*h/2))\n",
        "    (x2,y2) = (int(xc+boxes[b][2]*w/2), int(yc+boxes[b][3]*h/2))\n",
        "    draw.rectangle([x1,y1,x2,y2], width=2)\n",
        "    draw.text((x1,y1), ds.labels.info.class_names[ds.labels[ind].numpy()[b]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZMcRLeQPHq6"
      },
      "outputs": [],
      "source": [
        "# Display the image and its bounding boxes\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg8rUpSWPJoK"
      },
      "source": [
        "**Note:** For optimal object detection model performance, it is often important for datasets to contain images with no annotations (See the 4th sample in the dataset above). Empty samples can be appended using:\n",
        "\n",
        "`ds.boxes.append(None)`\n",
        "\n",
        "or by specifying an empty array whose len(shape) is equal to that of the other samples in the tensor:\n",
        "\n",
        "`ds.boxes.append(np.zeros(0,4)) #len(sample.shape) == 2`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79QnkE-UUySP"
      },
      "source": [
        "Congrats! You just created a beautiful object detection dataset! ðŸŽ‰"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Creating Object Detection Datasets",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}